{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# These vars represent the cropped image shape\n",
    "H = 60 \n",
    "W = 256\n",
    "\n",
    "# Pool = 4\n",
    "# PoolSize = 2\n",
    "# LastFilter = 256\n",
    "\n",
    "UnitClass = 30 # Number of characters\n",
    "MaxInputCharLen = 24 # Max label length of input character\n",
    "MaxPredictedCharLen = 64 # Max label length of predicted character\n",
    "CTCTime = MaxPredictedCharLen - 2\n",
    "\n",
    "BATCH_SIZE = 375 # x = train_size/batch_size -> (n-epoch/x ----- )\n",
    "EPOCH = 2\n",
    "TRAIN_SIZE = 30000 # Number of training sets that is used. \n",
    "VALID_SIZE = 3000 # Number of validation sets that is used.\n",
    "TEST_SIZE = 100 # Number of testing sets that is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable for directory\n",
    "def image_dir(base_img):\n",
    "    train_dir = base_img + '\\\\train_v2\\\\train'\n",
    "    test_dir = base_img + '\\\\test_v2\\\\test'\n",
    "    valid_dir = base_img + '\\\\validation_v2\\\\validation'\n",
    "    \n",
    "    return train_dir, test_dir, valid_dir\n",
    "\n",
    "def csv_dir(base_csv):\n",
    "    train_csv_dir = base_csv + '/written_name_train_v2.csv'\n",
    "    test_csv_dir = base_csv + '/written_name_test_v2.csv'\n",
    "    valid_csv_dir = base_csv + '/written_name_validation_v2.csv'\n",
    "    \n",
    "    return train_csv_dir, test_csv_dir, valid_csv_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DATASET\n",
    "def read_csv_dir(train, test, valid):\n",
    "    df_train = pd.read_csv(train)\n",
    "    df_test = pd.read_csv(test)\n",
    "    df_valid = pd.read_csv(valid)\n",
    "    \n",
    "    return df_train, df_test, df_valid\n",
    "\n",
    "# Check nan data in dataframe\n",
    "def nan_data(dataframe, *args):\n",
    "    \"\"\" \n",
    "    params:\n",
    "        dataframe -> data\n",
    "        *args -> label\n",
    "    \"\"\"\n",
    "    num_args=len(args)\n",
    "    \n",
    "    final_res = []\n",
    "    for data in dataframe:\n",
    "        bool_res_ = []\n",
    "        for i in range(num_args):\n",
    "            bool_res = True if ( data.isna().sum()[args[i]] != 0 ) else False\n",
    "            bool_res_.append(bool_res)\n",
    "            \n",
    "        if bool_res_.__contains__(True):\n",
    "            final_res.append(True)\n",
    "        else:\n",
    "            final_res.append(False)\n",
    "        \n",
    "        bool_res_.clear()\n",
    "        \n",
    "    return final_res\n",
    "\n",
    "# Drop nan data\n",
    "def drop_nan(dataframe, axis=0, inplace=False):\n",
    "    if not inplace:\n",
    "        df = dataframe.dropna(axis=axis, inplace=inplace)\n",
    "        return df\n",
    "    else:\n",
    "        dataframe.dropna(axis=axis, inplace=inplace)\n",
    "        return None\n",
    "\n",
    "# Reset index of data in dataframe\n",
    "def _reset_index_(dataframe, inplace=False, drop=False):\n",
    "    if not inplace:\n",
    "        df = dataframe.reset_index(inplace=inplace, drop=drop)\n",
    "        return df\n",
    "    else:\n",
    "        dataframe.reset_index(inplace=inplace, drop=drop)\n",
    "        return None \n",
    "\n",
    "def crop_image(image, dim=(64,128)):\n",
    "    (h, w) = image.shape # Check the input image (old-size image)\n",
    "    \n",
    "    if h > dim[0]:\n",
    "        image = image[:dim[0], :]\n",
    "    \n",
    "    if w > dim[1]:\n",
    "        image = image[:, :dim[1]]\n",
    "    \n",
    "    new_img = np.ones(dim)*255 # create white blank image\n",
    "    \n",
    "    new_img[:h, :w] = image # fill the array to new image\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "# Label the character of the input name to num or vice versa\n",
    "def label_name(name, name_to_num=True):\n",
    "    alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ'- \"\n",
    "    \n",
    "    label = []\n",
    "    if name_to_num:\n",
    "        name = name.upper()\n",
    "        for chars in name:\n",
    "            num = alphabets.find(chars)\n",
    "            label.append(num)\n",
    "        return np.array(label)\n",
    "    \n",
    "    else:\n",
    "        chars=\"\"\n",
    "        for nums in name:\n",
    "            chars += alphabets[nums]\n",
    "        return chars\n",
    "\n",
    "# Funct to label each char of the name in dataframe\n",
    "def label(df, size, max_char_len, label_base):\n",
    "    # Create new Label (output label) for dataset padded with the label of each char of the name\n",
    "    y_ = np.ones([size, max_char_len])*-1\n",
    "    # This var counts the length of the name. \n",
    "    y_len = np.zeros([size, 1])\n",
    "    \n",
    "    # Label all the training set\n",
    "        # The remaining unlabeled pads will be padded with -1\n",
    "    for i in range(size):\n",
    "        y_len[i] = len(df.loc[i, label_base])\n",
    "        y_[i, :len(df.loc[i, label_base])] = label_name(name=df.loc[i, label_base])\n",
    "    \n",
    "    return y_, y_len\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout\n",
    "\n",
    "# BatchNormalization : a technique for training very deep neural networks that normalizes the contributions to a layer for every mini-batch. This has the impact of settling the learning process and drastically decreasing the number of training epochs required to train deep neural networks.\n",
    "# https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf\n",
    "\n",
    "\n",
    "# target_shape_y = int(conf.H / math.pow(conf.PoolSize, conf.Pool))\n",
    "# target_shape_x = int(conf.W / math.pow(conf.PoolSize, conf.Pool))*conf.LastFilter\n",
    "\n",
    "inputs = Input(shape=(H, W, 1), name='INPUT')\n",
    "\n",
    "# 6 Conv Layer 2 RNN variation used here is Bidirectional LSTM.\n",
    "def build_model():\n",
    "    conv1 = Conv2D(32, kernel_size=(3,3), padding='same', name='CONV1')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2,2), name='POOL1')(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, kernel_size=(3,3), padding='same', name='CONV2')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2,2), name='POOL2')(conv2)\n",
    "    pool2 = Dropout(0.2)(pool2)\n",
    "    \n",
    "    conv3 = Conv2D(128, kernel_size=(3,3), padding='same', name='CONV3')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2,1), name='POOL3')(conv3)\n",
    "    pool3 = Dropout(0.2)(pool3)\n",
    "    \n",
    "    # layers = Conv2D(256, kernel_size=(3,3), padding='same', kernel_initializer=initializers.HeNormal, name='CONV4')(layers)\n",
    "    # layers = BatchNormalization()(layers)\n",
    "    # layers = Activation('relu')(layers)\n",
    "    \n",
    "    # pool4 = MaxPooling2D(pool_size=(1,2), name='POOL4')(pool3)\n",
    "    # pool4 = Dropout(0.2)(pool4)\n",
    "    \n",
    "    # conv4 = Conv2D(64, kernel_size=(3,3), padding='same', name='CONV4')(pool3)\n",
    "    # conv4 = BatchNormalization()(conv4)\n",
    "    # conv4 = Activation('relu')(conv4)\n",
    "    \n",
    "    # pool4 = MaxPooling2D(pool_size=(1,2), name='POOL5s')(conv4)\n",
    "    # pool4 = Dropout(0.2)(pool4)\n",
    "    \n",
    "    # #RNN\n",
    "    layers = Reshape(target_shape=(64, 896))(pool3)\n",
    "    layers = Dense(64, activation='relu', name='DENSE1')(layers)\n",
    "\n",
    "    layers = Bidirectional(LSTM(128, return_sequences=True, name='LSTM'))(layers)\n",
    "    layers = Bidirectional(LSTM(128, return_sequences=True, name='LSTM2'))(layers)\n",
    "    # layers = Bidirectional(LSTM(128, return_sequences=True, name='LSTM3'))(layers)\n",
    "    \n",
    "    layers = Dense(UnitClass)(layers)\n",
    "    y_pred = Activation('softmax')(layers)\n",
    "    # y_pred = Dense(conf.UnitClass, activation='softmax', kernel_initializer=initializers.HeNormal, name='OUTPUT')(layers)\n",
    "    # y_pred = Bidirectional(LSTM(const.UnitClass, activation='softmax'))(layers)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=y_pred).summary()\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "# Since RNNs are powerful for sequence learning, they require pre-segmented training data, \n",
    "# Each character in the label needs to be aligned to it location of occurrence in the input image.\n",
    "# Post-processing techniques are required on the output of RNN, which is a probability matrix, to transform it to the actual sequence of labels. \n",
    "# Connectionist Temporal Classification (CTC) is used to get those jobs above done. \n",
    "def ctc_loss_func(args):\n",
    "    y_pred, y_true, input_len, label_len = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN tend to be garbage.\n",
    "    y_pred = y_pred[:,2:,:]\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "\n",
    "def sgd_optimizer():\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    return sgd\n",
    "\n",
    "# Create new input label for ctc\n",
    "def input_label():\n",
    "    labels = Input(name='labels', shape=[MaxInputCharLen], dtype='float32')\n",
    "    input_len = Input(shape=[1], dtype='int64')\n",
    "    label_len= Input(shape=[1], dtype='int64')\n",
    "    return labels, input_len, label_len\n",
    "\n",
    "# This func to define a loss function\n",
    "def loss(y_pred, labels, input_len, label_len, name):\n",
    "    ctc = Lambda(ctc_loss_func, output_shape=(1,), name=name)([y_pred, labels, input_len, label_len]) \n",
    "    return ctc\n",
    "\n",
    "# Define crnn model with ctc implementation as output\n",
    "    # CTC is to interpret outputs of RNN as a probability distribution over all possible label sequences.\n",
    "# def CRNN():\n",
    "#     _, y_pred = build_model() # Gets the output of the model\n",
    "#     labels, input_len, label_len = input_label() # Create new labels for input\n",
    "#     # Define CTC\n",
    "#     ctc = loss(y_pred, labels, input_len, label_len, name='ctc') \n",
    "#     # Define Model \n",
    "#     model_ = Model(\n",
    "#         inputs=[inputs, labels, input_len, label_len],\n",
    "#         outputs=ctc\n",
    "#     )\n",
    "#     return model_, y_pred\n",
    "\n",
    "def train(model, x_train, x_valid, y_train, y_valid, train_input_len, train_label_len, train_output, valid_input_len, valid_label_len, valid_output, opt, y_pred):\n",
    "    model.compile(\n",
    "        loss={\n",
    "            'ctc': lambda y_true, y_pred: y_pred\n",
    "        },\n",
    "        optimizer=opt\n",
    "    )\n",
    "    \n",
    "    model.fit(x=[x_train, y_train, train_input_len, train_label_len], y=train_output,\n",
    "              validation_data=([x_valid, y_valid, valid_input_len, valid_label_len], valid_output),\n",
    "              epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "def validation(model, x_, size):\n",
    "    # try predict the data\n",
    "    pred = model.predict(x_)\n",
    "    # var to sum up the total of the given input\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    decoded = K.get_value(K.ctc_decode(pred, input_length=input_len)[0][0])\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(size):\n",
    "        decoded_name = label_name(decoded[i], name_to_num=False)\n",
    "        preds.append(decoded_name)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def validation_accuracy_metrics(data, preds, size, features):\n",
    "    valid_feature_names = data.loc[:size, features]\n",
    "    \n",
    "    corr_char = 0 # init var to check if the model has guessed correctly the character of each names\n",
    "    total_char = 0\n",
    "    for i in range(size): # loop validation set\n",
    "        # Check and compare decoded predicted name with the true name of the validation set \n",
    "        valid_fname_length = len(valid_feature_names[i])\n",
    "        preds_fname_length = len(preds[i])\n",
    "        total_char += valid_fname_length\n",
    "        for j in range(min(valid_fname_length, preds_fname_length)):\n",
    "            if valid_fname_length[j] == preds_fname_length[j]:\n",
    "                corr_char += 1\n",
    "    \n",
    "    print('total character : ', total_char)\n",
    "    char_predict_accuracy = float(corr_char*100/total_char)\n",
    "    return char_predict_accuracy\n",
    "\n",
    "def testing(model, test_dir, df, X_test, size):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    for i in range(size):\n",
    "        # show test image\n",
    "        img_dir = os.path.join(test_dir, df.loc[i, 'FILENAME'])\n",
    "        img = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # show predicted names\n",
    "        pred = model.predict(X_test)\n",
    "        input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "        decoded = K.get_value(K.ctc_decode(pred, input_length=input_len)[0][0]) # decode the predicted names\n",
    "        \n",
    "        plt.title(label_name(decoded[0], name_to_num=False)) # Convert the decoded to be name string\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 11 11:21:40 2022\n",
    "\n",
    "@author: user\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def IMGCrop(path, size, data):\n",
    "    img_arr = []\n",
    "    for i in range(size):\n",
    "        img_dir = os.path.join(path, data.loc[i, 'FILENAME'])\n",
    "        img = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE) # Grayscale channel 1\n",
    "        img = crop_image(img, dim=(H, W))\n",
    "        # cv2.imshow('image', img)\n",
    "        # cv2.waitKey(0) \n",
    "        # cv2.destroyAllWindows()\n",
    "        img = img/255.\n",
    "        img_arr.append(img)\n",
    "    \n",
    "    img_arr = np.array(img_arr)\n",
    "    return img_arr\n",
    "        \n",
    "\n",
    "    \n",
    "BASE = 'C:\\\\Clarenti\\\\Data\\\\Project\\\\ML\\\\Program\\\\Dataset\\\\Recognition\\\\Handwriting'\n",
    "\n",
    "# DEFINE VARIABLES\n",
    "train_dir, test_dir, valid_dir = image_dir(BASE)\n",
    "train_csv_dir, test_csv_dir, valid_csv_dir = csv_dir(BASE)\n",
    "\n",
    "# READ CSV\n",
    "df_train, df_test, df_valid = read_csv_dir(train_csv_dir, test_csv_dir, valid_csv_dir)\n",
    "print(df_train.shape)\n",
    "print(df_valid.shape)\n",
    "\n",
    "# CHECK FOR NAN DATA\n",
    "is_nan = nan_data([df_train, df_valid], 'FILENAME', 'IDENTITY')\n",
    "print({'train_nan':is_nan[0], 'valid_nan':is_nan[1]})\n",
    "\n",
    "# DROP NAN\n",
    "drop_nan(df_train, inplace=True)\n",
    "drop_nan(df_valid, inplace=True)\n",
    "\n",
    "# REMOVE INVALID DATA\n",
    "df_train = df_train[df_train['IDENTITY']!='UNREADABLE']\n",
    "df_valid = df_valid[df_valid['IDENTITY']!='UNREADABLE']\n",
    "\n",
    "print('CURRENT TRAIN : ', df_train.shape)\n",
    "print('CURRENT_VALID : ', df_valid.shape)\n",
    "\n",
    "# RESET INDEX ON DATAFRAME AFTER FILTERING\n",
    "_reset_index_(df_train, inplace=True, drop=True)\n",
    "_reset_index_(df_valid, inplace=True, drop=True)\n",
    "\n",
    "print(df_train.tail(5))\n",
    "\n",
    "#==========================================================    \n",
    "# IMAGE PREPROCESSING\n",
    "\n",
    "# CROP IMAGE\n",
    "X_train = IMGCrop(train_dir, TRAIN_SIZE, df_train)\n",
    "X_valid = IMGCrop(valid_dir, VALID_SIZE, df_valid)\n",
    "X_test = IMGCrop(test_dir, TEST_SIZE, df_test)\n",
    "\n",
    "print('CURRENT TRAIN : ', X_train.shape)\n",
    "print('CURRENT VALID : ', X_valid.shape)\n",
    "\n",
    "# RESHAPE\n",
    "X_train = np.array(X_train).reshape(-1, H, W, 1) # -1 -> =value digabung/diwrap jadi 1\n",
    "X_valid = np.array(X_valid).reshape(-1, H, W, 1)\n",
    "X_test = np.array(X_test).reshape(-1, H, W, 1)\n",
    "\n",
    "print('CURRENT TRAIN AFTER RESHAPE : ', X_train.shape)\n",
    "print('CURRENT VALID AFTER RESHAPE : ', X_valid.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BUILD AND TRAIN \n",
    "model, pred = build_model()\n",
    "# model, pred = CRNN()\n",
    "labels, input_len, label_len = input_label() # Create new labels for input\n",
    "# Define CTC\n",
    "ctc = loss(pred, labels, input_len, label_len, name='ctc') \n",
    "# Define Model \n",
    "model_ = Model(\n",
    "    inputs=[inputs, labels, input_len, label_len],\n",
    "    outputs=ctc\n",
    ")\n",
    "opt = sgd_optimizer()\n",
    "\n",
    "# ===================================================\n",
    "y_train, y_train_len = label(df_train, TRAIN_SIZE, MaxInputCharLen, 'IDENTITY')\n",
    "y_valid, y_valid_len = label(df_valid, VALID_SIZE, MaxInputCharLen, 'IDENTITY')\n",
    "\n",
    "train_input_len, valid_input_len = np.ones([TRAIN_SIZE, 1]) * CTCTime, np.ones([VALID_SIZE, 1]) * CTCTime\n",
    "train_output = np.zeros([TRAIN_SIZE])\n",
    "valid_output = np.zeros([VALID_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT\n",
    "train(model_, X_train, X_valid, y_train, y_valid, train_input_len, y_train_len, train_output, valid_input_len, y_valid_len, valid_output, opt, pred)\n",
    "    \n",
    "# ==========================================================\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "valid_preds = validation(model, X_valid, VALID_SIZE)\n",
    "valid_acc = validation_accuracy_metrics(df_valid, valid_preds, VALID_SIZE, 'IDENTITY')\n",
    "print('correct chars predicted : %f' %(valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TESTING\n",
    "testing(model, test_dir, df_test, X_test, 10)\n",
    "# test_acc = CRNN.validation_accuracy_metrics(df_valid, valid_preds, conf.VALID_SIZE, 'IDENTITY')\n",
    "# print('correct chars predicted : %f' %(valid_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbf1d3937cd9af2ec7a992e3a66647a7d94267f9ee808a22d5489bdc356721a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
